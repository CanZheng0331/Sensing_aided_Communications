{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CanZheng0331/Sensing_aided_Communications/blob/main/Computer_Vision_Aided_Beam_Tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Computer Vision Aided Beam Tracking**\n",
        "## **计算机视觉辅助波束追踪**\n",
        "\n",
        "This is a Python code package related to the following article: S. Jiang and A. Alkhateeb, \"**Computer Vision Aided Beam Tracking** in A Real-World Millimeter Wave Deployment,\" in IEEE Globecom Workshops, 2022.\n",
        "\n",
        "The original open sourced code is in: https://github.com/acyiobs/vision_beam_tracking/tree/main.\n",
        "\n",
        "Most of the annotations are written in Chinese."
      ],
      "metadata": {
        "id": "8KSegDu0KR__"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GV8A4feUD5Ba"
      },
      "outputs": [],
      "source": [
        "## 下载 场景8 数据集、视觉数据标注（YOLO V4 识别的 2D 边界框的坐标和属性）并解压\n",
        "\n",
        "!wget -q -O scenario8.zip \"https://www.dropbox.com/scl/fi/6yhd4xddx7d2zk4mcllq6/scenario8.zip?rlkey=w472s91e9tvv5p78yszkxmww3&e=1&dl=0\"\n",
        "!wget -q -O camera_data_bbox.zip \"https://www.dropbox.com/scl/fi/n1cqbxvpzxl9j4zhhgs3q/camera_data_bbox.zip?dl=0&e=1&rlkey=cibk7natbsm2axb8gzrvz12rl\"\n",
        "!unzip scenario8.zip\n",
        "!unzip camera_data_bbox.zip -d \"/content/DEV[95%]/unit1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_model_summary"
      ],
      "metadata": {
        "id": "bUsbfOcfLsja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import copy\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import ast\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from pytorch_model_summary import summary\n",
        "import sys\n",
        "import datetime\n",
        "from scipy.io import savemat\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "99nvGDncGNBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 序列生成\n",
        "## Sequence Generator\n",
        "\n",
        "# 阻塞预测函数\n",
        "def blockage_prediction(csv_frame, data_list_y):\n",
        "    data_label = np.empty((0, 1), dtype=int)\n",
        "    for i in np.arange(len(data_list_y)):\n",
        "        blockage = np.zeros(data_list_y[0])\n",
        "        for j in range(len(data_list_y[i])):\n",
        "            blockage[j] = csv_frame[csv_frame.index == data_list_y[i][j]]['blockage'].item()\n",
        "        blockage = np.sum(blockage) > 0\n",
        "        data_label = np.append(data_label, blockage)\n",
        "    return data_label\n",
        "\n",
        "# 波束预测函数：该函数的目标是预测 给定 时刻或给定数据条件下最合适的波束索引\n",
        "def beam_prediction(csv_frame, data_list_y):\n",
        "    data_label = np.empty((0, 1), dtype=int)\n",
        "    for i in tqdm(np.arange(len(data_list_y))):\n",
        "        power_levels = np.loadtxt(csv_frame[csv_frame.index == data_list_y[i][0]]['unit1_pwr_60ghz'].item())\n",
        "        beam_idx = np.argmax(power_levels) + 1\n",
        "        data_label = np.append(data_label, beam_idx)\n",
        "    return data_label\n",
        "\n",
        "# 波束追踪函数：该函数的目标是追踪 多个 时刻或多个位置下的波束选择，并且追踪的是一个时间序列或多个索引对应的波束\n",
        "def beam_tracking(csv_frame, data_list_y):\n",
        "    data_label = np.empty((len(data_list_y), len(data_list_y[0])), dtype=int)\n",
        "    base_path = '/content/scenario8/unit1/mmWave_data/'\n",
        "    for i in tqdm(np.arange(len(data_list_y))):\n",
        "        beam_idx = np.zeros(len(data_list_y[0]))\n",
        "        for j in range(len(data_list_y[i])):\n",
        "            file_name = csv_frame[csv_frame.index == data_list_y[i][j]]['unit1_pwr_60ghz'].item()\n",
        "            power_levels =  os.path.join(base_path, file_name)\n",
        "            beam_idx[j] = np.argmax(power_levels) + 1\n",
        "        data_label[i] = beam_idx\n",
        "    return data_label\n",
        "\n",
        "\n",
        "class TimeSeriesGenerator:\n",
        "    def __init__(self,\n",
        "                 csv_file='scenario8.csv',\n",
        "                 x_size=5,\n",
        "                 y_size=1,\n",
        "                 delay=0,\n",
        "                 seed=5,\n",
        "                 label_function=beam_prediction,\n",
        "                 save_filename=None):\n",
        "\n",
        "        # x_size: Size of input samples\n",
        "        # y_size: Size of label samples to generate the labels\n",
        "        # delay: The number of samples between x and y sequences.\n",
        "\n",
        "        # Example Series: x_size=3, y_size=1, delay=0 --> [1 2 3] [4]\n",
        "        # Example Series: x_size=3, y_size=1, delay=1 --> [1 2 3] [5]\n",
        "        # Example Series: x_size=3, y_size=2, delay=0 --> [1 2 3] [4, 5]\n",
        "\n",
        "        # For Beam Prediction, y_size=1\n",
        "        # For Blockage Prediction, y_size=# of samples in a blockage duration (e.g., 3 samples)\n",
        "\n",
        "        if save_filename == None:\n",
        "            self.save_filename = csv_file.split('.')[0] + '_series' + '.csv'\n",
        "        else:\n",
        "            self.save_filename = save_filename\n",
        "\n",
        "        self.csv_frame = pd.read_csv(csv_file, index_col='index')\n",
        "        self.num_sequences = self.csv_frame['seq_index'].max()\n",
        "\n",
        "        self.seq_start = []\n",
        "        self.seq_end = []\n",
        "        self._extract_seq_start_end()\n",
        "\n",
        "        self.x_size = x_size\n",
        "        self.y_size = y_size\n",
        "        self.delay = delay\n",
        "\n",
        "        self.data_list_x = np.empty((0, x_size), dtype=int)\n",
        "        self.data_list_y = np.empty((0, y_size), dtype=int)\n",
        "        self.data_list_seq = np.empty((0, y_size), dtype=int)\n",
        "\n",
        "        self._generate_indices()\n",
        "\n",
        "        self.data_labels = label_function(self.csv_frame, self.data_list_y)\n",
        "\n",
        "        # Shuffling indices\n",
        "        self.num_datapoints = len(self.data_list_y)\n",
        "        self.data_idx = np.arange(self.num_datapoints)\n",
        "        rng = np.random.default_rng(seed)\n",
        "        rng.shuffle(self.data_idx)\n",
        "\n",
        "        # Shuffling sequences\n",
        "        self.seq_idx = np.arange(self.num_sequences)\n",
        "        rng = np.random.default_rng(seed)\n",
        "        rng.shuffle(self.seq_idx)\n",
        "\n",
        "    def _extract_seq_start_end(self):\n",
        "        for i in np.arange(self.num_sequences) + 1:\n",
        "            data_indices = self.csv_frame[self.csv_frame['seq_index'] == i].index\n",
        "            self.seq_start.append(data_indices.min())\n",
        "            self.seq_end.append(data_indices.max())\n",
        "\n",
        "    def _generate_indices(self):\n",
        "        for i in range(len(self.seq_start)):\n",
        "            x_start_ind = self.seq_start[i]\n",
        "            x_end_ind = x_start_ind + self.x_size\n",
        "\n",
        "            y_start_ind = x_end_ind + self.delay\n",
        "            y_end_ind = y_start_ind + self.y_size\n",
        "\n",
        "            while y_end_ind <= self.seq_end[i] + 1:\n",
        "                self.data_list_x = np.vstack((self.data_list_x, np.arange(x_start_ind, x_end_ind)))\n",
        "                self.data_list_y = np.vstack((self.data_list_y, np.arange(y_start_ind, y_end_ind)))\n",
        "                self.data_list_seq = np.append(self.data_list_seq, i)\n",
        "                x_start_ind += 1\n",
        "                x_end_ind += 1\n",
        "                y_start_ind += 1\n",
        "                y_end_ind += 1\n",
        "\n",
        "    def take(self, num_of_data):\n",
        "        new_dataset = copy.copy(self)\n",
        "        new_dataset.data_idx = new_dataset.data_idx[:num_of_data]\n",
        "        new_dataset.num_datapoints = len(new_dataset.data_idx)\n",
        "        return new_dataset\n",
        "\n",
        "    def skip(self, num_of_data):\n",
        "        new_dataset = copy.copy(self)\n",
        "        new_dataset.data_idx = new_dataset.data_idx[num_of_data:]\n",
        "        new_dataset.num_datapoints = len(new_dataset.data_idx)\n",
        "        return new_dataset\n",
        "\n",
        "    def take_by_idx(self, idx):\n",
        "        new_dataset = copy.copy(self)\n",
        "        new_dataset.data_idx = new_dataset.data_idx[idx]\n",
        "        new_dataset.num_datapoints = len(new_dataset.data_idx)\n",
        "        return new_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_datapoints\n",
        "\n",
        "    def save_split_files(self, split=(0.7, 0.2, 0.1), data_path_csv_column=None, label_path_csv_column=None,\n",
        "                         split_names=('train', 'val', 'test'), label_name='beam_index', sequence_split=False,\n",
        "                         save_y_ind=False):\n",
        "        if sequence_split:\n",
        "            num_sequences = self.num_sequences\n",
        "            num_train = int(num_sequences * split[0])\n",
        "            num_val = int(num_sequences * split[1])\n",
        "            idx_train = np.where(np.in1d(self.data_list_seq[self.data_idx], self.seq_idx[:num_train]))\n",
        "            idx_val = np.where(np.in1d(self.data_list_seq[self.data_idx], self.seq_idx[num_train:num_train + num_val]))\n",
        "            idx_test = np.where(np.in1d(self.data_list_seq[self.data_idx], self.seq_idx[num_train + num_val:]))\n",
        "\n",
        "\n",
        "        else:\n",
        "            # Train, Validation, Test\n",
        "            num_datapoints = len(self)\n",
        "            num_train = int(num_datapoints * split[0])\n",
        "            num_val = int(num_datapoints * split[1])\n",
        "            idx_train = np.arange(0, num_train)\n",
        "            idx_val = np.arange(num_train, num_train + num_val)\n",
        "            idx_test = np.arange(num_train + num_val, num_datapoints)\n",
        "\n",
        "        idx_list = [idx_train, idx_val, idx_test]\n",
        "        for n, name in enumerate(split_names):\n",
        "            self.take_by_idx(idx_list[n]).save_file(file_tag=name, data_path_csv_column=data_path_csv_column,\n",
        "                                                    label_path_csv_column=label_path_csv_column, label_name=label_name,\n",
        "                                                    shuffled=True, save_y_ind=save_y_ind)\n",
        "        '''\n",
        "        self.take_by_idx(idx_train).save_file(file_tag=split_names[0], data_path_csv_column=data_path_csv_column, label_path_csv_column=label_path_csv_column, label_name=label_name, shuffled=True, save_y_ind=False)\n",
        "        self.take_by_idx(idx_val).save_file(file_tag=split_names[1], data_path_csv_column=data_path_csv_column, label_path_csv_column=label_path_csv_column, label_name=label_name, shuffled=True, save_y_ind=False)\n",
        "        self.take_by_idx(idx_test).save_file(file_tag=split_names[2], data_path_csv_column=data_path_csv_column, label_path_csv_column=label_path_csv_column, label_name=label_name, shuffled=True,save_y_ind=False)\n",
        "        '''\n",
        "\n",
        "    # data_path_csv_column: If the location of the sequences are required\n",
        "    # in the output csv file, input the name of the csv column\n",
        "    # (e.g., 'unit1_radar').\n",
        "    def save_file(self, file_tag='', data_path_csv_column=None, label_path_csv_column=None, label_name='label',\n",
        "                  shuffled=False, save_y_ind=False):\n",
        "        if data_path_csv_column is None:\n",
        "            df_x = pd.DataFrame(self.data_list_x, columns=['x_%i' % (i + 1) for i in range(self.x_size)])\n",
        "        else:\n",
        "            df_x = pd.DataFrame(self.csv_frame[data_path_csv_column].to_numpy(str)[self.data_list_x - 1],\n",
        "                                columns=['x_%i' % (i + 1) for i in range(self.x_size)])\n",
        "        if save_y_ind:\n",
        "            if label_path_csv_column is None:\n",
        "                df_y = pd.DataFrame(self.data_list_y, columns=['y_%i' % (i + 1) for i in range(self.y_size)])\n",
        "            else:\n",
        "                df_y = pd.DataFrame(self.csv_frame[label_path_csv_column].to_numpy(str)[self.data_list_y - 1],\n",
        "                                    columns=['y_%i' % (i + 1) for i in range(self.y_size)])\n",
        "        else:\n",
        "            df_y = pd.DataFrame()\n",
        "        df_label = pd.DataFrame(self.data_labels,\n",
        "                                columns=['%s_%i' % (label_name, i + 1) for i in range(self.data_labels.shape[1])])\n",
        "        df = pd.concat([df_x, df_y, df_label], axis=1)\n",
        "        df.index.name = 'index'\n",
        "        df.index += 1\n",
        "\n",
        "        if shuffled:\n",
        "            df = df.iloc[self.data_idx]\n",
        "            df.index.name = 'data_index'\n",
        "            df = df.reset_index()\n",
        "            df.index.name = 'index'\n",
        "            df.index += 1\n",
        "\n",
        "        filename = self.save_filename.split('.')[0] + '_' + file_tag + '.csv'\n",
        "        df.to_csv(filename)\n",
        "        print('%i data points are saved to %s' % (len(df), filename))\n",
        "\n",
        "\n",
        "# %% Generate Time Series & Save Series CSV Files\n",
        "\n",
        "csv_file = '/content/DEV[95%]/scenario8.csv'\n",
        "\n",
        "# May define your own label extraction function if needed\n",
        "# blockage_prediction and beam_prediction are currently available\n",
        "label_function = beam_tracking\n",
        "\n",
        "# Data sequence size\n",
        "x_size = 8\n",
        "# Label sequence size, beam_prediction --> 1, blockage_prediction --> 3\n",
        "y_size = x_size + 5\n",
        "# Delay\n",
        "delay = -x_size\n",
        "\n",
        "rng_seed = 5  # Reproducibility\n",
        "\n",
        "# The file name will be included in the series\n",
        "data_path_csv_column = 'unit1_rgb'\n",
        "label_path_csv_column = 'unit1_pwr_60ghz'\n",
        "save_y_ind = True\n",
        "# Name of the labels\n",
        "label_name = 'beam_index'\n",
        "\n",
        "# Sequence or data split of the files\n",
        "# If False, the data is fully shuffled\n",
        "# If True, Train-Validation-Test sets are separated by the sequence\n",
        "# i.e., any of the sets will not have a shared sequence\n",
        "sequence_split = True\n",
        "\n",
        "x = TimeSeriesGenerator(csv_file=csv_file,\n",
        "                        x_size=x_size,\n",
        "                        y_size=y_size,\n",
        "                        seed=rng_seed,\n",
        "                        delay=delay,\n",
        "                        label_function=label_function,\n",
        "                        save_filename='/content/DEV[95%]/scenario8_series.csv')\n",
        "\n",
        "x.save_file(file_tag='full', data_path_csv_column=data_path_csv_column, label_path_csv_column=label_path_csv_column,\n",
        "            label_name=label_name, shuffled=False, save_y_ind=save_y_ind)\n",
        "x.save_split_files(split=(0.8, 0.0, 0.2), data_path_csv_column=data_path_csv_column,\n",
        "                   label_path_csv_column=label_path_csv_column, label_name=label_name, sequence_split=sequence_split,\n",
        "                   save_y_ind=save_y_ind)"
      ],
      "metadata": {
        "id": "xmtuOGDgG-eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4oCf8rRHCdG"
      },
      "outputs": [],
      "source": [
        "## Pre-Processing\n",
        "\n",
        "def preprocess(in_path, out_path):\n",
        "    base_path = '/content/DEV[95%]'\n",
        "    csv_file_path = in_path\n",
        "    csv_save_path = out_path\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    cols1 = ['x_'+str(s) for s in range(1,9)]\n",
        "    cols2 = ['y_'+str(s) for s in range(1,14)]\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        for c in cols1:\n",
        "            relative_path = row.loc[c]\n",
        "            relative_path = relative_path.replace('camera_data', 'camera_data_bbox').replace('jpg', 'txt')\n",
        "            full_path = os.path.join(base_path, relative_path)\n",
        "            path = os.path.normpath(os.path.join(base_path, relative_path.lstrip('./')))\n",
        "            try:\n",
        "                content = np.loadtxt(path)[1:]\n",
        "            except:\n",
        "                content = np.zeros(4)\n",
        "            if not content.size:\n",
        "                content = np.zeros(4)\n",
        "            df.at[index, c] = np.array2string(content, separator=',')\n",
        "        for c in cols2:\n",
        "            relative_path = row.loc[c]\n",
        "            path = os.path.normpath(os.path.join(base_path, relative_path.lstrip('./')))\n",
        "            content = np.loadtxt(path)\n",
        "            df.at[index, c] = np.array2string(content, separator=',')\n",
        "\n",
        "    df.to_csv(csv_save_path, index=False)\n",
        "\n",
        "\n",
        "#%% training dataset\n",
        "csv_file_path = '/content/DEV[95%]/scenario8_series_train.csv'\n",
        "csv_save_path = '/content/DEV[95%]/scenario8_series_bbox_train.csv'\n",
        "preprocess(csv_file_path, csv_save_path)\n",
        "\n",
        "#%% test dataset\n",
        "csv_file_path = '/content/DEV[95%]/scenario8_series_test.csv'\n",
        "csv_save_path = '/content/DEV[95%]/scenario8_series_bbox_test.csv'\n",
        "preprocess(csv_file_path, csv_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPqG80cyPqhd"
      },
      "outputs": [],
      "source": [
        "## Data Feeder\n",
        "\n",
        "def create_samples(root, portion=1.0, num_beam=64):\n",
        "    f = pd.read_csv(root)\n",
        "    bbox_all = []\n",
        "    beam_power_all = []\n",
        "    for idx, row in f.iterrows():\n",
        "        bboxes = np.stack(\n",
        "            [np.asarray(ast.literal_eval(r)) for r in row.loc[\"x_1\":\"x_8\"]], axis=0\n",
        "        )\n",
        "        bbox_all.append(bboxes)\n",
        "        beam_powers = np.stack(\n",
        "            [np.asarray(ast.literal_eval(r)) for r in row.loc[\"y_1\":\"y_13\"]], axis=0\n",
        "        )\n",
        "        beam_power_all.append(beam_powers)\n",
        "\n",
        "    bbox_all = np.stack(bbox_all, axis=0)\n",
        "    beam_power_all = np.stack(beam_power_all, axis=0)\n",
        "    best_beam = np.argmax(beam_power_all, axis=-1)\n",
        "\n",
        "    print(\"list is ready\")\n",
        "    num_data = len(beam_power_all)\n",
        "    num_data = int(num_data * portion)\n",
        "    return bbox_all[:num_data], best_beam[:num_data, -5:]\n",
        "\n",
        "\n",
        "class DataFeed(Dataset):\n",
        "    def __init__(self, root_dir, portion=1.0, num_beam=64):\n",
        "\n",
        "        self.root = root_dir\n",
        "        self.samples, self.pred_val = create_samples(\n",
        "            self.root, portion=portion, num_beam=num_beam\n",
        "        )\n",
        "        self.seq_len = 8\n",
        "        self.num_beam = num_beam\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        samples = self.samples[idx]  # Read one data sample\n",
        "        pred_val = self.pred_val[idx]\n",
        "\n",
        "        samples = samples[-self.seq_len :]  # Read a sequence of tuples from a sample\n",
        "\n",
        "        out_beam = torch.zeros((5,))\n",
        "        bbox = torch.zeros((self.seq_len, 4))\n",
        "\n",
        "        if not samples.size:\n",
        "            samples = np.zeros(4)\n",
        "        bbox = torch.tensor(samples, requires_grad=False)\n",
        "\n",
        "        out_beam = torch.tensor(pred_val, requires_grad=False)\n",
        "\n",
        "        return bbox.float(), out_beam.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0UuGtcyP0XR"
      },
      "outputs": [],
      "source": [
        "## Model\n",
        "\n",
        "class GruModelSimple(nn.Module):\n",
        "    def __init__(self, num_classes, num_layers=3, hidden_size=64, embed_size=64):\n",
        "        super(GruModelSimple, self).__init__()\n",
        "        self.embed = torch.nn.Linear(4, embed_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = torch.nn.GRU(\n",
        "            input_size=embed_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=0.3,\n",
        "        )\n",
        "        self.fc = torch.nn.Linear(hidden_size, num_classes)\n",
        "        self.name = \"GruModelSimple\"\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros((self.num_layers, batch_size, self.hidden_size))\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        y = self.embed(x)\n",
        "        y = self.dropout1(y)\n",
        "        y, h = self.gru(y, h)\n",
        "        y = self.fc(y)\n",
        "        return y, h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkfdFMkbP3q4"
      },
      "outputs": [],
      "source": [
        "## Training\n",
        "\n",
        "def train_model(num_epoch=100, if_writer=False, portion=1.0, num_beam=64):\n",
        "    num_classes = num_beam + 1\n",
        "    batch_size = 8\n",
        "    val_batch_size = 64\n",
        "    train_dir = \"/content/DEV[95%]/scenario8_series_bbox_train.csv\"\n",
        "    val_dir = \"/content/DEV[95%]/scenario8_series_bbox_test.csv\"\n",
        "    train_loader = DataLoader(\n",
        "        DataFeed(train_dir, num_beam=num_beam), batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        DataFeed(val_dir, num_beam=num_beam), batch_size=val_batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    # check gpu acceleration availability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    now = datetime.datetime.now().strftime(\"%H_%M_%S\")\n",
        "    date = datetime.date.today().strftime(\"%y_%m_%d\")\n",
        "\n",
        "    # Instantiate the model\n",
        "    net = GruModelSimple(num_classes)\n",
        "    # path to save the model\n",
        "    checkpoint_dir = \"./checkpoint/\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    PATH = os.path.join(checkpoint_dir, f\"{now}_{date}_{net.name}.pth\")\n",
        "    #PATH = \"./checkpoint/\" + now + \"_\" + date + \"_\" + net.name + \"\" + \".pth\"\n",
        "    # print model summary\n",
        "    if if_writer:\n",
        "        h = net.initHidden(1)\n",
        "        print(summary(net, torch.zeros((8, 1, 4)), h))\n",
        "    # send model to GPU\n",
        "    net.to(device)\n",
        "\n",
        "    # set up loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer, milestones=[10, 30, 50], gamma=0.1)\n",
        "    if if_writer:\n",
        "        writer = SummaryWriter(comment=now + \"_\" + date + \"_\" + net.name)\n",
        "        # writer.add_graph(net, (torch.zeros((1, 1, 1024)), torch.zeros((1, 1, 1024))))\n",
        "\n",
        "    # train model\n",
        "    val_top1_acc_his = []\n",
        "    val_top2_acc_his = []\n",
        "    val_top3_acc_his = []\n",
        "    val_top5_acc_his = []\n",
        "\n",
        "    for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        running_acc = 1.0\n",
        "        with tqdm(train_loader, unit=\"batch\", file=sys.stdout) as tepoch:\n",
        "            for i, (bbox, label) in enumerate(tepoch, 0):\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                # get the input np arrays, bbox sequence (batch_size, 8, 4) label (batch_size, 1)\n",
        "                #bbox = (bbox - bbox.mean(dim=1, keepdim=True)) / bbox.std(dim=1, keepdim=True)\n",
        "                bbox = torch.swapaxes(bbox, 0, 1)\n",
        "                bbox = torch.cat(\n",
        "                    [bbox, torch.zeros(torch.Size((4,)) + bbox.shape[1:]) - 1], dim=0\n",
        "                )\n",
        "                label = torch.swapaxes(label, 0, 1)\n",
        "                bbox = bbox.to(device)\n",
        "                label = label.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                h = net.initHidden(bbox.shape[1]).to(device)\n",
        "                outputs, _ = net(bbox, h)\n",
        "                outputs = outputs[-5:, ...]\n",
        "                loss = criterion(outputs.view(-1, num_classes), label.flatten())\n",
        "                prediction = torch.argmax(outputs, dim=-1)\n",
        "                acc = (prediction == label).sum().item() / int(\n",
        "                    torch.sum(label != -100).cpu()\n",
        "                )\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                # print statistics\n",
        "                running_loss = (loss.item() + i * running_loss) / (i + 1)\n",
        "                running_acc = (acc + i * running_acc) / (i + 1)\n",
        "                log = OrderedDict()\n",
        "                log[\"loss\"] = running_loss\n",
        "                log[\"acc\"] = running_acc\n",
        "                tepoch.set_postfix(log)\n",
        "            scheduler.step()\n",
        "            # validation\n",
        "            predictions = []\n",
        "            net.eval()\n",
        "            with torch.no_grad():\n",
        "                total = np.zeros((5,))\n",
        "                top1_correct = np.zeros((5,))\n",
        "                top2_correct = np.zeros((5,))\n",
        "                top3_correct = np.zeros((5,))\n",
        "                top5_correct = np.zeros((5,))\n",
        "\n",
        "                val_loss = 0\n",
        "                for (bbox, label) in val_loader:\n",
        "                    bbox = torch.swapaxes(bbox, 0, 1)\n",
        "                    #bbox = (bbox - bbox.mean(dim=1, keepdim=True)) / bbox.std(dim=1, keepdim=True)\n",
        "                    bbox = torch.cat(\n",
        "                        [bbox, torch.zeros(torch.Size((4,)) + bbox.shape[1:]) - 1],\n",
        "                        dim=0,\n",
        "                    )\n",
        "                    label = torch.swapaxes(label, 0, 1)\n",
        "                    bbox = bbox.to(device)\n",
        "                    label = label.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    h = net.initHidden(bbox.shape[1]).to(device)\n",
        "                    outputs, _ = net(bbox, h)\n",
        "                    outputs = outputs[-5:, ...]\n",
        "                    label = label[-5:, ...]\n",
        "                    val_loss += nn.CrossEntropyLoss(reduction=\"sum\")(\n",
        "                        outputs.view(-1, num_classes), label.flatten()\n",
        "                    ).item()\n",
        "                    total += torch.sum(label != -100, dim=-1).cpu().numpy()\n",
        "                    prediction = torch.argmax(outputs, dim=-1)\n",
        "                    top1_correct += torch.sum(prediction == label, dim=-1).cpu().numpy()\n",
        "                    _, idx = torch.topk(outputs, 5, dim=-1)\n",
        "                    idx = idx.cpu().numpy()\n",
        "                    idx = np.minimum(idx, num_beam - 1)\n",
        "                    label = label.cpu().numpy()\n",
        "                    for i in range(label.shape[0]):\n",
        "                        for j in range(label.shape[1]):\n",
        "                            top2_correct[i] += np.isin(label[i, j], idx[i, j, :2]).sum()\n",
        "                            top3_correct[i] += np.isin(label[i, j], idx[i, j, :3]).sum()\n",
        "                            top5_correct[i] += np.isin(label[i, j], idx[i, j, :5]).sum()\n",
        "\n",
        "                    predictions.append(prediction.cpu().numpy())\n",
        "\n",
        "                val_loss /= total.sum()\n",
        "                #scheduler.step(val_loss)\n",
        "                val_top1_acc = top1_correct / total\n",
        "                val_top2_acc = top2_correct / total\n",
        "                val_top3_acc = top3_correct / total\n",
        "                val_top5_acc = top5_correct / total\n",
        "\n",
        "                print(\"val_loss={:.4f}\".format(val_loss), flush=True)\n",
        "                print(\"accuracy\", flush=True)\n",
        "                print(\n",
        "                    np.stack(\n",
        "                        [val_top1_acc, val_top2_acc, val_top3_acc, val_top5_acc], 0\n",
        "                    ),\n",
        "                    flush=True,\n",
        "                )\n",
        "                print(\"power\", flush=True)\n",
        "        if if_writer:\n",
        "            writer.add_scalar(\"Loss/train\", running_loss, epoch)\n",
        "            writer.add_scalar(\"Loss/test\", val_loss, epoch)\n",
        "            writer.add_scalar(\"acc/train\", running_acc, epoch)\n",
        "            writer.add_scalar(\"acc/test\", val_top1_acc[0], epoch)\n",
        "        val_top1_acc_his.append(val_top1_acc)\n",
        "        val_top2_acc_his.append(val_top2_acc)\n",
        "        val_top3_acc_his.append(val_top3_acc)\n",
        "        val_top5_acc_his.append(val_top5_acc)\n",
        "\n",
        "    if if_writer:\n",
        "        writer.close()\n",
        "        torch.save(net.state_dict(), PATH)\n",
        "\n",
        "    his = {\n",
        "        \"acc_top1\": val_top1_acc_his,\n",
        "        \"acc_top2\": val_top2_acc_his,\n",
        "        \"acc_top3\": val_top3_acc_his,\n",
        "        \"acc_top5\": val_top5_acc_his,\n",
        "    }\n",
        "    print(\"Finished Training\")\n",
        "\n",
        "    # load the model\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    # test\n",
        "    predictions = []\n",
        "    raw_predictions = []\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        total = np.zeros((5,))\n",
        "        top1_correct = np.zeros((5,))\n",
        "        top2_correct = np.zeros((5,))\n",
        "        top3_correct = np.zeros((5,))\n",
        "        top5_correct = np.zeros((5,))\n",
        "        val_loss = 0\n",
        "        for (bbox, label) in val_loader:\n",
        "            bbox = torch.swapaxes(bbox, 0, 1)\n",
        "            bbox = torch.cat(\n",
        "                [bbox, torch.zeros(torch.Size((4,)) + bbox.shape[1:]) - 1], dim=0\n",
        "            )\n",
        "            label = torch.swapaxes(label, 0, 1)\n",
        "            bbox = bbox.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            bbox = bbox.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            h = net.initHidden(bbox.shape[1]).to(device)\n",
        "            outputs, _ = net(bbox, h)\n",
        "            outputs = outputs[-5:, ...]\n",
        "            label = label[-5:, ...]\n",
        "            val_loss += nn.CrossEntropyLoss(reduction=\"sum\")(\n",
        "                outputs.view(-1, num_classes), label.flatten()\n",
        "            ).item()\n",
        "            total += torch.sum(label != -100, dim=-1).cpu().numpy()\n",
        "            prediction = torch.argmax(outputs, dim=-1)\n",
        "            top1_correct += torch.sum(prediction == label, dim=-1).cpu().numpy()\n",
        "\n",
        "            _, idx = torch.topk(outputs, 5, dim=-1)\n",
        "            idx = idx.cpu().numpy()\n",
        "            label = label.cpu().numpy()\n",
        "            for i in range(label.shape[0]):\n",
        "                for j in range(label.shape[1]):\n",
        "                    top2_correct[i] += np.isin(label[i, j], idx[i, j, :2]).sum()\n",
        "                    top3_correct[i] += np.isin(label[i, j], idx[i, j, :3]).sum()\n",
        "                    top5_correct[i] += np.isin(label[i, j], idx[i, j, :5]).sum()\n",
        "\n",
        "            predictions.append(prediction.cpu().numpy())\n",
        "            raw_predictions.append(outputs.cpu().numpy())\n",
        "\n",
        "        val_loss /= total.sum()\n",
        "        val_top1_acc = top1_correct / total\n",
        "        val_top2_acc = top2_correct / total\n",
        "        val_top3_acc = top3_correct / total\n",
        "        val_top5_acc = top5_correct / total\n",
        "\n",
        "        predictions = np.concatenate(predictions, 1)\n",
        "        raw_predictions = np.concatenate(raw_predictions, 1)\n",
        "\n",
        "        val_acc = {\n",
        "            \"top1\": val_top1_acc,\n",
        "            \"top2\": val_top2_acc,\n",
        "            \"top3\": val_top3_acc,\n",
        "            \"top5\": val_top5_acc,\n",
        "        }\n",
        "        return val_loss, val_acc, predictions, raw_predictions, his\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)\n",
        "    num_epoch = 100\n",
        "    val_loss, val_acc, predictions, raw_predictions, his = train_model(\n",
        "        num_epoch=num_epoch, if_writer=True, portion=1.0, num_beam=64\n",
        "    )\n",
        "    print(val_acc)\n",
        "\n",
        "    plot_dir = \"./plot\"\n",
        "    os.makedirs(plot_dir, exist_ok=True)\n",
        "\n",
        "    file_path = os.path.join(plot_dir, \"test_acc.mat\")\n",
        "    savemat(file_path, {'test_acc': val_acc})\n",
        "    print(f\"Results saved at: {file_path}\")\n",
        "\n",
        "    savemat('plot/test_acc.mat',{'test_acc':val_acc})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNlQlf0OUuk+P7uX5rebHXe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}